{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('data/final-corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(corpus, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus contains several comments from YouTube videos, and each comment may have one or more relevant aspects that it discusses. Each aspect is defined to also have a target.\n",
    "\n",
    "In the CSV, each row consists of a target, and all aspects related to that target. Thus, the comments are repeated across rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     yes one thing if he is president of usa have b...\n",
       "1     yes one thing if he is president of usa have b...\n",
       "2     please safe this great nation and vote no to l...\n",
       "3     elizabeth warren is so on point , she even has...\n",
       "4     i hate it when candidates try to pretend to be...\n",
       "5     i hate it when candidates try to pretend to be...\n",
       "6     what presidents actually have control over is ...\n",
       "7     warren is smelling corporate money , see how t...\n",
       "8     i love her < 3 ive been feelin the bern since ...\n",
       "9     i love her < 3 ive been feelin the bern since ...\n",
       "10    you would think anyone claiming to be native a...\n",
       "11                           `` persist '' go elizabeth\n",
       "12    bernie served in the house from 1991 until the...\n",
       "13    but she would n't let indians go to college as...\n",
       "14    every democrat i 've met so far has some type ...\n",
       "15    while warren and other democrats are f ' n aro...\n",
       "16                                          crazy women\n",
       "17    her credibility on her claims to native herita...\n",
       "18    yep lol look at her thumbnail pic folks ... wh...\n",
       "19    so glad to see people on cnn arent even buying...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head(20).comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can evaluate the model in several different ways \n",
    "\n",
    "- Can we predict any/all of the targets that the comment contains?\n",
    "- Can we predict any/all of the aspects the comment talks about?\n",
    "- Can we predict any/all of the sub-aspects the comment talks about?\n",
    "- Can we predict (aspect, target) pairs?\n",
    "- Can we predict (sub-aspect, target) pairs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the evaluation subroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "def flatten(nested):\n",
    "    return list(it.chain.from_iterable(nested))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_all(predictions, references):\n",
    "    return set(predictions) == set(references)\n",
    "\n",
    "def match_any(predictions, references):\n",
    "    p = predictions\n",
    "    r = references\n",
    "    i = p.intersection(r)\n",
    "    return len(i) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accuracy(predictions, gold_standards, must_match_all):\n",
    "    \"\"\"\n",
    "    Calculates accuracy based on \"any matches\"/\"all matches\" rule.\n",
    "    \"\"\"\n",
    "    scoring_function = match_all if must_match_all else match_any\n",
    "    return np.mean([\n",
    "        scoring_function(pred, golds)\n",
    "        for pred, golds in zip(predictions, gold_standards)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_counts_comment(prediction, reference):\n",
    "    tp = 0; fp = 0; fn = 0\n",
    "    for p in prediction:\n",
    "        if p not in reference:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tp += 1\n",
    "    for r in reference:\n",
    "        if r not in prediction:\n",
    "            fn += 1\n",
    "    return tp, fp, fn\n",
    "\n",
    "def pos_neg_counts(predictions, references):\n",
    "    tp = 0; fp = 0; fn = 0\n",
    "    for i, (p, r) in enumerate(zip(predictions, references)):\n",
    "        _tp, _fp, _fn = pos_neg_counts_comment(p, r)\n",
    "        tp += _tp\n",
    "        fp += _fp\n",
    "        fn += _fn\n",
    "    return tp, fp, fn\n",
    "\n",
    "def precision(tp, fp, fn):\n",
    "    return tp/(tp + fp)\n",
    "\n",
    "def recall(tp, fp, fn):\n",
    "    return tp/(fp + fn)\n",
    "\n",
    "def f1(p, r):\n",
    "    num = 2*p*r\n",
    "    den = p+r\n",
    "    return num/den\n",
    "\n",
    "def fdr(tp, fp, fn):\n",
    "    return fp/(fp+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, gold_standards, should_print=False):\n",
    "    \"\"\"\n",
    "    predictions: iterable of iterables of predictions\n",
    "    gold_standards: iterable of iterables of gold standard values\n",
    "    \"\"\"\n",
    "    any_acc = custom_accuracy(predictions, gold_standards, must_match_all=False)\n",
    "    all_acc = custom_accuracy(predictions, gold_standards, must_match_all=True)\n",
    "    tp, fp, fn = pos_neg_counts(predictions, gold_standards)\n",
    "    prec = precision(tp, fp, fn)\n",
    "    rec = recall(fp, tp, fn)\n",
    "    fdr_score = fdr(tp, fp, fn) \n",
    "    f1_score = f1(prec, rec)\n",
    "\n",
    "    if should_print:\n",
    "        print('\"Match any\" accuracy: {}'.format(round(any_acc, 3)))\n",
    "        print('\"Match all\" acc: {}'.format(round(all_acc, 3)))\n",
    "        print('Precision: {}'.format(round(prec, 3)))\n",
    "        print('Recall: {}'.format(round(rec, 3)))\n",
    "        print('F1 score: {}'.format(round(f1_score, 3)))\n",
    "        print('FDR: {}'.format(round(fdr_score, 3)))\n",
    "    return any_acc, all_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries: Constructing the gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_gold_standard_exp1(data):\n",
    "    out = []\n",
    "    for _, target_series in data.groupby('comment_id').target:\n",
    "        out.append(set(target_series))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_predictions(y_pred, comment_id):\n",
    "    df = pd.DataFrame()\n",
    "    df['comment_id'] = comment_id\n",
    "    df['y_pred'] = y_pred\n",
    "    out= []\n",
    "    for _, pred_series in df.groupby('comment_id').y_pred:\n",
    "        out.append(set(pred_series))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_gold_train = construct_gold_standard_exp1(train)\n",
    "exp1_gold_test = construct_gold_standard_exp1(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries: Constructing BoW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cv.fit_transform(train.comment)\n",
    "X_test = cv.transform(test.comment)\n",
    "\n",
    "y_train = train.target\n",
    "y_test = test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Can we identify the target\n",
    "\n",
    "Here we could try a simple logistic regression at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_pred_train = construct_predictions(y_pred_train, train.comment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_pred_test = construct_predictions(y_pred_test, test.comment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "\"Match any\" accuracy: 0.998\n",
      "\"Match all\" acc: 0.943\n",
      "Precision: 0.998\n",
      "Recall: 0.002\n",
      "F1 score: 0.005\n",
      "FDR: 0.002\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "_ = evaluate(exp1_pred_train, exp1_gold_train, should_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "\"Match any\" accuracy: 0.593\n",
      "\"Match all\" acc: 0.574\n",
      "Precision: 0.593\n",
      "Recall: 0.396\n",
      "F1 score: 0.475\n",
      "FDR: 0.407\n"
     ]
    }
   ],
   "source": [
    "print('Test set:')\n",
    "_ = evaluate(exp1_pred_test, exp1_gold_test, should_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.fit(exp1_gold_train + exp1_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unique = cv.transform(train.comment.unique())\n",
    "X_test_unique = cv.transform(test.comment.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_unique = mlb.transform(exp1_gold_train)\n",
    "y_test_unique = mlb.transform(exp1_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_rf_y_train = mlb.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_unique, y_train_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unique_train = rf.predict(X_train_unique)\n",
    "y_pred_unique_test = rf.predict(X_test_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_rf_pred_train = [set(t) for t in mlb.inverse_transform(y_pred_unique_train)]\n",
    "exp1_rf_pred_test = [set(t) for t in mlb.inverse_transform(y_pred_unique_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "\"Match any\" accuracy: 0.995\n",
      "\"Match all\" acc: 0.995\n",
      "Precision: 1.0\n",
      "Recall: 0.0\n",
      "F1 score: 0.0\n",
      "FDR: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "_ = evaluate(exp1_rf_pred_train, exp1_gold_train, should_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "\"Match any\" accuracy: 0.13\n",
      "\"Match all\" acc: 0.111\n",
      "Precision: 0.333\n",
      "Recall: 0.252\n",
      "F1 score: 0.287\n",
      "FDR: 0.667\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "_ = evaluate(exp1_rf_pred_test, exp1_gold_test, should_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this performing so horribly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'yang'},\n",
       " {'warren'},\n",
       " {'sanders'},\n",
       " {'biden'},\n",
       " set(),\n",
       " {'biden'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'sanders', 'warren'},\n",
       " {'yang'},\n",
       " {'warren'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'biden'},\n",
       " {'warren'},\n",
       " {'biden'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'biden'},\n",
       " set(),\n",
       " {'biden'},\n",
       " set(),\n",
       " {'biden'},\n",
       " set(),\n",
       " set(),\n",
       " {'warren'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'warren'},\n",
       " {'buttigieg'},\n",
       " set(),\n",
       " set(),\n",
       " {'biden'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'warren'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'yang'},\n",
       " {'biden'},\n",
       " set(),\n",
       " {'yang'},\n",
       " set(),\n",
       " set(),\n",
       " {'biden'},\n",
       " set(),\n",
       " set(),\n",
       " {'biden'},\n",
       " {'biden'},\n",
       " {'warren'},\n",
       " set(),\n",
       " {'biden'},\n",
       " set(),\n",
       " set(),\n",
       " {'yang'},\n",
       " {'buttigieg'},\n",
       " {'biden'},\n",
       " {'buttigieg'},\n",
       " {'biden'},\n",
       " {'biden'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'biden'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'biden', 'yang'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'warren'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'biden'},\n",
       " set(),\n",
       " {'warren'},\n",
       " {'yang'},\n",
       " {'biden'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp1_rf_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-ha, a lot of the predictions are empty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.5: Can we jointly predict (target, general) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_gold_standard_exp15(data):\n",
    "    out= []\n",
    "    for cid, row in train.groupby('comment_id'):\n",
    "        tgts = row.target.tolist()\n",
    "        gens = row.general.tolist()\n",
    "        out.append({f\"{tgt}_{gen}\" for tgt, gen in zip(tgts, gens)})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp15_gold_train = construct_gold_standard_exp15(train)\n",
    "exp15_gold_test = construct_gold_standard_exp15(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_exp15 = train.apply(lambda row: f\"{row.target}_{row.general}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_exp15 = test.apply(lambda row: f\"{row.target}_{row.general}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y=y_train_exp15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_exp15 = logreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_exp15 = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp15_pred_train = construct_predictions(y_pred_train_exp15, train.comment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp15_pred_test = construct_predictions(y_pred_test_exp15, test.comment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "\"Match any\" accuracy: 0.988\n",
      "\"Match all\" acc: 0.933\n",
      "Precision: 0.988\n",
      "Recall: 0.012\n",
      "F1 score: 0.023\n",
      "FDR: 0.012\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "_ = evaluate(exp15_pred_train, exp15_gold_train, should_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "\"Match any\" accuracy: 0.12\n",
      "\"Match all\" acc: 0.111\n",
      "Precision: 0.12\n",
      "Recall: 0.826\n",
      "F1 score: 0.21\n",
      "FDR: 0.88\n"
     ]
    }
   ],
   "source": [
    "print('Test set:')\n",
    "_ = evaluate(exp15_pred_test, exp15_gold_test, should_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this clearly isn't learning anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: {'sanders_positive'}\n",
      "Actual: {'sanders_positive'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'sanders_positive'}\n",
      "Actual: {'sanders_positive', 'warren_positive'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'sanders_positive'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'sanders_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'warren_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'warren_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'sanders_positive'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'warren_nan'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'sanders_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'sanders_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'biden_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'yang_positive'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'buttigieg_positive'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'buttigieg_positive'}\n",
      "Actual: {'sanders_positive'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'yang_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'sanders_positive', 'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_positive'}\n",
      "Actual: {'yang_positive'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'biden_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_positive'}\n",
      "Actual: {'sanders_positive', 'warren_positive'}\n",
      "\n",
      "Predicted: {'yang_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'yang_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'buttigieg_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'yang_negative', 'biden_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_nan'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_nan'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'yang_positive'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_negative'}\n",
      "Actual: {'sanders_negative', 'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'yang_negative'}\n",
      "Actual: {'biden_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'sanders_negative', 'biden_positive'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'sanders_positive', 'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'biden_positive'}\n",
      "Actual: {'warren_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'sanders_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'warren_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_nan'}\n",
      "\n",
      "Predicted: {'biden_positive'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'yang_positive'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'buttigieg_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_positive'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'sanders_positive'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'warren_nan'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'yang_positive'}\n",
      "\n",
      "Predicted: {'warren_negative'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'buttigieg_negative'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'biden_nan'}\n",
      "Actual: {'buttigieg_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_positive'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_positive'}\n",
      "\n",
      "Predicted: {'warren_nan'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_negative'}\n",
      "\n",
      "Predicted: {'biden_negative'}\n",
      "Actual: {'buttigieg_positive'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for comm, pred, gold in zip(test.comment.unique(), exp15_pred_test, exp15_gold_test):\n",
    "    print(f'Predicted: {pred}\\nActual: {gold}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.fit(exp15_gold_train + exp15_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_unique_exp15 = mlb.transform(exp15_gold_train)\n",
    "y_test_unique_exp15 = mlb.transform(exp15_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_unique, y_train_unique_exp15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unique_train_exp15 = rf.predict(X_train_unique)\n",
    "y_pred_unique_test_exp15 = rf.predict(X_test_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp15_rf_pred_train = [set(t) for t in mlb.inverse_transform(y_pred_unique_train_exp15)]\n",
    "exp15_rf_pred_test = [set(t) for t in mlb.inverse_transform(y_pred_unique_test_exp15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "\"Match any\" accuracy: 0.995\n",
      "\"Match all\" acc: 0.995\n",
      "Precision: 1.0\n",
      "Recall: 0.0\n",
      "F1 score: 0.0\n",
      "FDR: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "_ = evaluate(exp15_rf_pred_train, exp15_gold_train, should_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "\"Match any\" accuracy: 0.028\n",
      "\"Match all\" acc: 0.019\n",
      "Precision: 0.097\n",
      "Recall: 0.243\n",
      "F1 score: 0.138\n",
      "FDR: 0.903\n"
     ]
    }
   ],
   "source": [
    "print('Test set:')\n",
    "_ = evaluate(exp15_rf_pred_test, exp15_gold_test, should_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, yet again a gross overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Can we predict the aspects and their polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_COLUMNS = ['policy_race', 'appeal_white', 'policy_unspecified', 'policy_other',\n",
    "       'policy_international', 'policy_healthcare', 'policy_economy',\n",
    "       'campaign_prospects', 'policy_education', 'policy_lgbt', 'appeal_old',\n",
    "       'appeal_african', 'appeal_democrat', 'appeal_other', 'appeal_young',\n",
    "       'appeal_unspecified', 'appeal_female', 'appeal_asian',\n",
    "       'appeal_hispanic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_gold_standard_exp2(data):\n",
    "    out = []\n",
    "    for cid, comment_df in train.fillna('').groupby('comment_id'):\n",
    "        asp = []\n",
    "        for col in POLICY_COLUMNS:\n",
    "            pol = comment_df[col].tolist()\n",
    "            for p in pol:\n",
    "                if p != \"\":\n",
    "                    asp.append(f\"{col}_{p}\")\n",
    "        if len(asp) == 0:\n",
    "            asp.append('general_only')\n",
    "        out.append(asp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_gold_train = construct_gold_standard_exp2(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_gold_test = construct_gold_standard_exp2(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Won't work here, since we truly have multi-label data that can't be converted properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.fit(exp2_gold_train + exp2_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_unique_exp2 = mlb.transform(exp2_gold_train)\n",
    "y_test_unique_exp2 = mlb.transform(exp2_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_unique, y_train_unique_exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unique_train_exp2 = rf.predict(X_train_unique)\n",
    "y_pred_unique_test_exp2 = rf.predict(X_test_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_rf_pred_train = [set(t) for t in mlb.inverse_transform(y_pred_unique_train_exp2)]\n",
    "exp2_rf_pred_test = [set(t) for t in mlb.inverse_transform(y_pred_unique_test_exp2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "\"Match any\" accuracy: 0.998\n",
      "\"Match all\" acc: 0.998\n",
      "Precision: 1.0\n",
      "Recall: 0.0\n",
      "F1 score: 0.0\n",
      "FDR: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "_ = evaluate(exp2_rf_pred_train, exp2_gold_train, should_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "\"Match any\" accuracy: 0.694\n",
      "\"Match all\" acc: 0.694\n",
      "Precision: 0.682\n",
      "Recall: 0.304\n",
      "F1 score: 0.421\n",
      "FDR: 0.318\n"
     ]
    }
   ],
   "source": [
    "print('Test set:')\n",
    "_ = evaluate(exp2_rf_pred_test, exp2_gold_test, should_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Can we predict (candidate, aspect) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_gold_standard_exp3(data):\n",
    "    out = []\n",
    "    for cid, comment_df in train.fillna('').groupby('comment_id'):\n",
    "        asp = []\n",
    "        for _, row in comment_df.iterrows():\n",
    "            tgt = row['target']\n",
    "            pols = [row[c] for c in POLICY_COLUMNS]\n",
    "            if all([p == \"\" for p in pols]):\n",
    "                asp.append((tgt, 'general_only'))\n",
    "            else:\n",
    "                for c in POLICY_COLUMNS:\n",
    "                    pol = row[c]\n",
    "                    if pol != \"\":\n",
    "                        asp.append((tgt, f\"{c}_{pol}\"))\n",
    "        out.append(asp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_gold_train = construct_gold_standard_exp3(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_gold_test = construct_gold_standard_exp3(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.fit(exp3_gold_train + exp3_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_unique_exp3 = mlb.transform(exp3_gold_train)\n",
    "y_test_unique_exp3 = mlb.transform(exp3_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_unique, y_train_unique_exp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_unique_train_exp3 = rf.predict(X_train_unique)\n",
    "y_pred_unique_test_exp3 = rf.predict(X_test_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_rf_pred_train = [set(t) for t in mlb.inverse_transform(y_pred_unique_train_exp3)]\n",
    "exp3_rf_pred_test = [set(t) for t in mlb.inverse_transform(y_pred_unique_test_exp3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "\"Match any\" accuracy: 0.998\n",
      "\"Match all\" acc: 0.998\n",
      "Precision: 0.998\n",
      "Recall: 0.002\n",
      "F1 score: 0.004\n",
      "FDR: 0.002\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "_ = evaluate(exp3_rf_pred_train, exp3_gold_train, should_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "\"Match any\" accuracy: 0.037\n",
      "\"Match all\" acc: 0.028\n",
      "Precision: 0.121\n",
      "Recall: 0.234\n",
      "F1 score: 0.16\n",
      "FDR: 0.879\n"
     ]
    }
   ],
   "source": [
    "print('Test set:')\n",
    "_ = evaluate(exp3_rf_pred_test, exp3_gold_test, should_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In line with previous findings, a crazy overfit!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
